# Cascade & allocate: A cross-structure adversarial attack against models fusing vision and language (Information Fusion 2024)
Official implementation of ["**[Cascade & allocate: A cross-structure adversarial attack against models fusing vision and language (Information Fusion 2024)](https://doi.org/10.1016/j.inffus.2023.102179)**"], **[Boyuan Zhang](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uBBaTjEAAAAJ&citation_for_view=uBBaTjEAAAAJ:pS0ncopqnHgC)**, **[Yucheng Shi](https://scholar.google.com/citations?user=annoZWEAAAAJ&hl=en)**, **[YaHong Han](http://cic.tju.edu.cn/faculty/hanyahong/index.html)**, **[Qinghua Hu](https://scholar.google.com/citations?user=TVSNq_wAAAAJ&hl=en)**, Weiping Ding.
DOI: https://doi.org/10.1016/j.inffus.2023.102179


> **Abstract:** *The data fusion systems between multiple modals have attracted a wide range of concerns about their adversarial robustness. Recent image captioning attacks lack cross-structure transferability against models with diverse structures. Therefore, the attackers demonstrate satisfactory performance only when they gain full or partial knowledge of target image captioning models. So far, the cross-structure transferability of multi-modal adversarial examples has not been thoroughly investigated. In this paper, a theorem is proposed to analyze the upper bound captioning adversarial transferability. We design a new transfer-based adversarial attack method (Cascade & Allocate) against image captioning models. Our method can be divided into two steps. The first step randomly selects a set of candidate models with diverse structures, and we use a momentum-like strategy to generate perturbations. This ‚Äòmodel cascade‚Äô step narrows the gap of the gradient directions between different image captioning models, therefore enhancing the cross-model transferability and generating model-agnostic adversarial perturbations. The second step utilizes the upper bound transferability theorem to allocate the weight of perturbations per model. This ‚Äòweight allocate‚Äô step enhances the weight of model with the most consistent gradient in all candidate models, which generates more transferable adversarial examples. In the experiments, we compare our approach with other captioning and ensemble-based attacks against five blackbox models with different structures on MS COCO and Flickr-30k datasets. The adversarial examples exhibit state-of-the-art adversarial transferability against five black-box models with different structures. The results demonstrate that our approach is practical and generalizes well to a wide range of captioning models.*

### The codes will be released once they are annotated.

## 1. Introduction

<p align="center">
    <img src='/Introduction.png' width=700/>
</p>

The Overview of our Cascade & Allocate method. The circles, squares, triangles, and rhombs represent image captioning models with different structures. The structure includes two steps: ‚ÄòModel Cascade‚Äô and ‚ÄòWeight Allocate‚Äô. The model cascade step can compress the adversarial perturbations and enhance the cross-model transferability. The weight allocate step measures the importance weight of perturbations per round. The final adversarial example generated by our framework can enhance the cross-structure transferability against different models at the same time.

## 2. Method

<p align="center">
    <img src='/Framework.png' width=800/>
</p>

Details of our Cascade & Allocate method. We only choose the perturbation generated from the final model each round, narrowing the gradient direction gap between different image captioning models. Then we measure the weight of perturbation and repeat *ùêº* rounds. In the final adversarial example, $x_adv$ can attack the target model with unknown structure.

## 3. Usage
### 3.1 Prepare data
The datasets used in the paper are available at the following links:
* [MS COCO](https://seungjunnah.github.io/Datasets/gopro.html)
* [Flickr-30k](https://github.com/joanshen0508/HA_deblur)
* [ImageNet](https://github.com/rimchang/RealBlur)


### 3.2 Dependencies

* Python 3.11
* PyTorch 1.13.0
* numpy
* Pillow
* torchvision
* scipy

### 3.3 Train

```
python attack.py --device 0 --dataset MS_coco --models ./models --rounds 40 --model_number 5
```

### Citation
If you find our code or paper useful, please consider citing:
```
@article{Cascade_Allocate,
  title = {Cascade \& {{Allocate}}: {{A Cross-Structure Adversarial Attack Against Models Fusing Vision}} and {{Language}}},
  shorttitle = {Cascade \& {{Allocate}}},
  author = {Zhang, Boyuan and Shi, Yucheng and Han, Yahong and Hu, Qinghua and Ding, Weiping},
  year = {2024},
  month = apr,
  journal = {Information Fusion},
  volume = {104},
  pages = {102179},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2023.102179},
}

```

### Acknowledgments

The code borrows heavily from [adversarial-attack-to-caption](https://github.com/wubaoyuan/adversarial-attack-to-caption). Thanks them very much for their sharing.
